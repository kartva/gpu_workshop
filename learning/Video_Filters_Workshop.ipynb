{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Filters Workshop\n",
        "\n",
        "Implement 2D convolution, then use it to apply filters to video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n"
          ]
        }
      ],
      "source": [
        "!pip install triton opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: 2D Convolution\n",
        "\n",
        "Let $r_y = \\frac{K_H - 1}{2}$ and $r_x = \\frac{K_W - 1}{2}$. Out-of-bounds accesses to $A$ are treated as zero.\n",
        "\n",
        "$$C[y, x] = \\sum_{i=0}^{K_H-1} \\sum_{j=0}^{K_W-1} A[y + i - r_y, x + j - r_x] \\cdot B[i, j]$$\n",
        "\n",
        "*Hint: if you're having trouble with arbitrary sizes, try hardcoding 3x3 kernels first.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def conv2d_kernel(\n",
        "    input_ptr, kernel_ptr, output_ptr,\n",
        "    H, W, KH, KW,\n",
        "    BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n",
        "):\n",
        "    # Your kernel here!\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv2d(image: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Apply convolution to a 2D image. Returns output of same size (with padding).\"\"\"\n",
        "    H, W = image.shape\n",
        "    KH, KW = kernel.shape\n",
        "    output = torch.empty_like(image)\n",
        "    \n",
        "    BLOCK_H, BLOCK_W = 16, 16\n",
        "    grid = (triton.cdiv(H, BLOCK_H), triton.cdiv(W, BLOCK_W))\n",
        "    \n",
        "    conv2d_kernel[grid](\n",
        "        image, kernel, output,\n",
        "        H, W, KH, KW,\n",
        "        BLOCK_H=BLOCK_H, BLOCK_W=BLOCK_W\n",
        "    )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your impl against torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "H, W = 256, 256\n",
        "test_image = torch.randn((H, W), device='cuda', dtype=torch.float32)\n",
        "kernel = torch.ones((3, 3), device='cuda', dtype=torch.float32) / 9.0  # try other kernels!\n",
        "\n",
        "your_output = conv2d(test_image, kernel)\n",
        "\n",
        "expected = F.conv2d(\n",
        "    test_image.unsqueeze(0).unsqueeze(0),\n",
        "    kernel.unsqueeze(0).unsqueeze(0),\n",
        "    padding=kernel.shape[0] // 2\n",
        ").squeeze()\n",
        "\n",
        "if torch.allclose(your_output, expected, atol=1e-4):\n",
        "    print(\"Yay!\")\n",
        "else:\n",
        "    print(f\"Max diff: {(your_output - expected).abs().max().item()}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "axes[0].imshow(test_image.cpu().numpy(), cmap='gray')\n",
        "axes[0].set_title(\"Input\")\n",
        "axes[1].imshow(your_output.cpu().numpy(), cmap='gray')\n",
        "axes[1].set_title(\"Your Output\")\n",
        "axes[2].imshow(expected.cpu().numpy(), cmap='gray')\n",
        "axes[2].set_title(\"Expected\")\n",
        "for ax in axes: ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Video Filters\n",
        "\n",
        "Now we'll apply these convolutions to each frame in a video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "from IPython.display import Video, display\n",
        "\n",
        "VIDEOS = {\n",
        "    \"bunny.mp4\": \"https://raw.githubusercontent.com/kartva/gpu_workshop/main/learning/videos/bunny.mp4\",\n",
        "    \"jellyfish.mp4\": \"https://raw.githubusercontent.com/kartva/gpu_workshop/main/learning/videos/jellyfish.mp4\"\n",
        "}\n",
        "\n",
        "for name, url in VIDEOS.items():\n",
        "    if not os.path.exists(name):\n",
        "        print(f\"Downloading {name}...\")\n",
        "        urllib.request.urlretrieve(url, name)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def apply_filter_to_video(input_path: str, output_path: str, kernel_r: torch.Tensor, kernel_g: torch.Tensor, kernel_b: torch.Tensor):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Write to temp file first, then re-encode for browser compatibility\n",
        "    temp_path = output_path.replace('.mp4', '_temp.mp4')\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(temp_path, fourcc, fps, (width, height), isColor=True)\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # BGR -> separate channels\n",
        "        frame_float = frame.astype(np.float32) / 255.0\n",
        "        b = torch.from_numpy(frame_float[:, :, 0]).cuda()\n",
        "        g = torch.from_numpy(frame_float[:, :, 1]).cuda()\n",
        "        r = torch.from_numpy(frame_float[:, :, 2]).cuda()\n",
        "\n",
        "        # Apply convolution to each channel\n",
        "        r_out = conv2d(r, kernel_r)\n",
        "        g_out = conv2d(g, kernel_g)\n",
        "        b_out = conv2d(b, kernel_b)\n",
        "\n",
        "        # Recombine\n",
        "        out_frame = np.stack([\n",
        "            b_out.cpu().numpy().clip(0, 1),\n",
        "            g_out.cpu().numpy().clip(0, 1),\n",
        "            r_out.cpu().numpy().clip(0, 1)\n",
        "        ], axis=-1)\n",
        "        out.write((out_frame * 255).astype(np.uint8))\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Processed {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    \n",
        "    # Re-encode with H.264 for browser compatibility\n",
        "    print(\"Re-encoding for browser playback...\")\n",
        "    subprocess.run(['ffmpeg', '-y', '-i', temp_path, '-c:v', 'libx264', '-preset', 'fast', output_path], \n",
        "                   capture_output=True)\n",
        "    os.remove(temp_path)\n",
        "    print(f\"Saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define kernels for each channel - try different ones!\n",
        "kernel_r = torch.ones((3, 3), device='cuda', dtype=torch.float32) / 9.0\n",
        "kernel_g = torch.ones((3, 3), device='cuda', dtype=torch.float32) / 9.0\n",
        "kernel_b = torch.ones((3, 3), device='cuda', dtype=torch.float32) / 9.0\n",
        "\n",
        "apply_filter_to_video(\"bunny.mp4\", \"bunny_filtered.mp4\", kernel_r, kernel_g, kernel_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Video(\"bunny_filtered.mp4\", embed=True, width=640))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
