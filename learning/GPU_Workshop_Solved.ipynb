{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hi! You should duplicate this Colab file so you can run and edit it on your own!\n",
        "\n",
        "# Part 1: ReLU"
      ],
      "metadata": {
        "id": "FrToNEaNSNJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ut2iSPdOP1k",
        "outputId": "82cf44f3-f198-4885-bf12-a9fea5270f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch"
      ],
      "metadata": {
        "id": "-o4yHYfSOYLi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def relu_kernel(in_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
        "  # get block id\n",
        "  pid = tl.program_id(axis=0)\n",
        "  block_start = pid * BLOCK_SIZE\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "  in_tile = tl.load(in_ptr + offsets)\n",
        "\n",
        "  tl.store(out_ptr + offsets, tl.maximum(in_tile, 0))"
      ],
      "metadata": {
        "id": "kpO5HULGOj8R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x: torch.Tensor) -> torch.Tensor:\n",
        "  output = torch.empty_like(x)\n",
        "  n_elements = x.numel()\n",
        "\n",
        "  BLOCK_SIZE = 256\n",
        "  grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
        "\n",
        "  relu_kernel[grid](x, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
        "  return output"
      ],
      "metadata": {
        "id": "m7J1VfYbPlvx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.randn(256 * 100, device=\"cuda\")\n",
        "your_output = relu(test_input)\n",
        "\n",
        "expected_output = torch.relu(test_input)\n",
        "\n",
        "# Check if they match\n",
        "if not torch.allclose(your_output, expected_output):\n",
        "    print(your_output, expected_output)\n",
        "else:\n",
        "    print(\"Yay!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiUUgZTHQDMF",
        "outputId": "b56bb6d0-ad0b-4518-d101-0cc85ab81764"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yay!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: MatMul"
      ],
      "metadata": {
        "id": "Zpl0eAJASbG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def matmul_kernel(A, B, C, M, N, K, MT: tl.constexpr, NT: tl.constexpr, KT: tl.constexpr):\n",
        "  c_m, c_n = tl.program_id(0), tl.program_id(1)\n",
        "  offset_m = c_m * MT + tl.arange(0, MT)\n",
        "  offset_n = c_n * NT + tl.arange(0, NT)\n",
        "  offset_k = tl.arange(0, KT)\n",
        "\n",
        "  acc = tl.zeros((MT, NT), dtype=C.dtype.element_ty)\n",
        "\n",
        "  for k in range(0, tl.cdiv(K, KT)):\n",
        "    k_step = k * KT + offset_k\n",
        "\n",
        "    a_ptrs = A + (offset_m[:, None] * K + k_step[None, :])\n",
        "    b_ptrs = B + (k_step[:, None] * N + offset_n[None, :])\n",
        "    acc = tl.dot(tl.load(a_ptrs), tl.load(b_ptrs), acc)\n",
        "\n",
        "  c_ptrs = C + (offset_m[:, None] * N + offset_n[None, :])\n",
        "  tl.store(c_ptrs, acc)\n",
        "\n",
        "def matmul(a: torch.Tensor, b: torch.Tensor):\n",
        "    M, K = a.shape\n",
        "    _, N = b.shape\n",
        "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
        "\n",
        "    MT, NT, KT = 128, 128, 32\n",
        "\n",
        "    grid = (triton.cdiv(M, MT), triton.cdiv(N, NT))\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        a, b, c,\n",
        "        M, N, K,\n",
        "        MT=MT, NT=NT, KT=KT\n",
        "    )\n",
        "    return c"
      ],
      "metadata": {
        "id": "-m8pXQSXQJNA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M, N, K = 512, 512, 512\n",
        "a = torch.randn((M, K), device='cuda')\n",
        "b = torch.randn((K, N), device='cuda')\n",
        "\n",
        "your_output = matmul(a, b)\n",
        "expected_output = torch.matmul(a, b)\n",
        "\n",
        "# Check if they match\n",
        "if not torch.allclose(your_output, expected_output):\n",
        "    print(your_output, expected_output)\n",
        "else:\n",
        "    print(\"Yay!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvvWX3zlukRi",
        "outputId": "8e4c256c-3d67-436e-bcfa-6e52d7f73d58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yay!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: MNIST Inference\n",
        "\n",
        "Now let's use our kernels to run a real neural network! We have a pre-trained 4-layer MLP:\n",
        "- **Layer 1:** 784 → 256 (input: flattened 28×28 image)\n",
        "- **Layer 2:** 256 → 128\n",
        "- **Layer 3:** 128 → 64\n",
        "- **Layer 4:** 64 → 10 (output: 10 digit classes)"
      ],
      "metadata": {
        "id": "fkk7tAfTvpud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Download pretrained weights from GitHub\n",
        "WEIGHTS_URL = \"https://raw.githubusercontent.com/kartva/gpu_workshop/main/learning/mnist_mlp_weights.pt\"\n",
        "WEIGHTS_PATH = \"mnist_mlp_weights.pt\"\n",
        "\n",
        "if not os.path.exists(WEIGHTS_PATH):\n",
        "    print(f\"Downloading weights...\")\n",
        "    urllib.request.urlretrieve(WEIGHTS_URL, WEIGHTS_PATH)\n",
        "    print(\"Done!\")\n",
        "\n",
        "weights = torch.load(WEIGHTS_PATH, map_location=\"cuda\", weights_only=True)\n",
        "\n",
        "print(\"Network Architecture:\")\n",
        "for name, tensor in weights.items():\n",
        "    print(f\"  {name:15} → {tuple(tensor.shape)}\")"
      ],
      "metadata": {
        "id": "tbfsS1IQuuhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x: torch.Tensor, weights: dict) -> torch.Tensor:\n",
        "    \"\"\"Run forward pass through the 4-layer MLP.\"\"\"\n",
        "    x = x.cuda().float()\n",
        "    if x.dim() == 1:\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "    # Layer 1: Linear + ReLU\n",
        "    w1 = weights[\"fc1.weight\"].T.contiguous()\n",
        "    b1 = weights[\"fc1.bias\"]\n",
        "    x = matmul(x, w1) + b1\n",
        "    x = relu(x)\n",
        "\n",
        "    # Layer 2: Linear + ReLU\n",
        "    w2 = weights[\"fc2.weight\"].T.contiguous()\n",
        "    b2 = weights[\"fc2.bias\"]\n",
        "    x = matmul(x, w2) + b2\n",
        "    x = relu(x)\n",
        "\n",
        "    # Layer 3: Linear + ReLU\n",
        "    w3 = weights[\"fc3.weight\"].T.contiguous()\n",
        "    b3 = weights[\"fc3.bias\"]\n",
        "    x = matmul(x, w3) + b3\n",
        "    x = relu(x)\n",
        "\n",
        "    # Layer 4: Linear (no activation)\n",
        "    w4 = weights[\"fc4.weight\"].T.contiguous()\n",
        "    b4 = weights[\"fc4.bias\"]\n",
        "    x = matmul(x, w4) + b4\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "forward_pass"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST test dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(test_dataset)} test images\")"
      ],
      "metadata": {
        "id": "load_mnist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Pick a random test image\n",
        "idx = random.randint(0, len(test_dataset) - 1)\n",
        "image, true_label = test_dataset[idx]\n",
        "\n",
        "# Flatten and run inference\n",
        "flat_image = image.view(1, 784)\n",
        "logits = forward(flat_image, weights)\n",
        "predicted_label = logits.argmax(dim=1).item()\n",
        "\n",
        "# Display results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "ax1.imshow(image.squeeze().numpy(), cmap=\"gray\")\n",
        "ax1.set_title(f\"True Label: {true_label}\", fontsize=14)\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "probs = torch.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "colors = [\"green\" if i == predicted_label else \"steelblue\" for i in range(10)]\n",
        "ax2.barh(range(10), probs, color=colors)\n",
        "ax2.set_yticks(range(10))\n",
        "ax2.set_xlabel(\"Probability\")\n",
        "ax2.set_title(f\"Predicted: {predicted_label}\", fontsize=14)\n",
        "ax2.set_xlim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if predicted_label == true_label:\n",
        "    print(f\"Correct! Predicted {predicted_label}\")\n",
        "else:\n",
        "    print(f\"Wrong! Predicted {predicted_label}, actual {true_label}\")"
      ],
      "metadata": {
        "id": "run_inference"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}